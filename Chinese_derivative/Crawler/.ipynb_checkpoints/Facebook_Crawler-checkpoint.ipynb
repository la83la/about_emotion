{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705b8e67",
   "metadata": {},
   "source": [
    "https://pypi.org/project/facebook-scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c938726b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from facebook_scraper import get_posts\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a723c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新聞網\n",
    "reporter_list=[ \n",
    "    \n",
    "    'pnnpts', # 公視\n",
    "    'twreporter', # 報導者 The Reporter\n",
    "    'myudn', # udn.com 聯合新聞網\n",
    "    'appledaily.tw', # 蘋果新聞網\n",
    "    'setnews', # 三立新聞\n",
    "    'ftvnews53', # 民視新聞\n",
    "    'ETtoday', # ETtoday新聞雲\n",
    "    'YahooTWNews', # Yahoo!奇摩新聞\n",
    "    'news.ebc', # 東森新聞\n",
    "    'TBSCTS', # 華視新聞\n",
    "    'TheNewsLens', # The News Lens 關鍵評論網\n",
    "    'tvbsfb', # TVBS 新聞\n",
    "    'CTfans', # 中時新聞網\n",
    "]\n",
    "\n",
    "# 財經網站\n",
    "finance_list = [ \n",
    "    'ebcmoney',  # 東森財經: 81萬追蹤\n",
    "    'moneyweekly',  # 理財周刊: 36.3萬\n",
    "    'cmoneyapp',  # CMoney 理財寶: 84.2萬\n",
    "    'emily0806',  # 艾蜜莉-自由之路: 20.9萬追蹤\n",
    "    'imoney889',  # 林恩如-飆股女王: 10.2萬\n",
    "    'wealth1974',  # 財訊: 17.5萬\n",
    "    'smart16888',  # 郭莉芳理財講堂: 1.6萬\n",
    "    'smartmonthly',  # Smart 智富月刊: 52.6萬\n",
    "    'MoneyMoneyMeg',  # Money錢: 20.7萬\n",
    "    'imoneymagazine',  # iMoney 智富雜誌: 38萬\n",
    "    'edigest',  # 經濟一週 EDigest: 36.2萬\n",
    "    'BToday',  # 今周刊:107萬\n",
    "    'GreenHornFans',  # 綠角財經筆記: 25萬\n",
    "    'ec.ltn.tw',  # 自由時報財經頻道 42,656人在追蹤\n",
    "    'MoneyDJ',  # MoneyDJ理財資訊 141,302人在追蹤\n",
    "    'YahooTWFinance',  # Yahoo奇摩股市理財 149,624人在追蹤\n",
    "    'anuetw',  # Anue鉅亨網財經新聞: 31.2萬追蹤\n",
    "]\n",
    "    \n",
    "# 公司\n",
    "company_list = [\n",
    "    'HonHai.Technology', # 鴻海科技集團Hon Hai Technology Group\n",
    "    'MediaTekTaiwan', # 聯發科\n",
    "    'delta.taiwan', # 台達 Delta\n",
    "    'FUBON', # 富邦 FUBON\n",
    "    'Advantech.Taiwan', # 研華台灣\n",
    "    'fareastone', # 遠傳電信\n",
    "    'TSMCRecruiterExpress', # 台積\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec12dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = (\"joe40515@yahoo.com.tw\", \"howard20224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544180be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_crawler(page_id, numOfPage, timeout= 15, credentials=credentials, \\\n",
    "               extra_info=True, \\\n",
    "               cookies='C:/Users/USER/Desktop/Prof_Hsieh_Project/about_emotion/Chinese_derivative/Crawler/cookies.json', \n",
    "               options={\"reactors\":True, \"posts_per_page\":1}):\n",
    "    \"\"\"\n",
    "    只能爬粉專\n",
    "    page_id: string, id for target page\n",
    "    numOfPage: int, number of pages\n",
    "    options: dict, allow reactor or not/number of posts per page\n",
    "    \"\"\"\n",
    "    # post crawler\n",
    "    listposts = []\n",
    "    count = 1\n",
    "    try: \n",
    "        for tmp in get_posts(page_id, pages=numOfPage, options=options):\n",
    "            print(f'Current domain: {page_id}')\n",
    "            print(f'Current Process: {count}/{numOfPage}...')\n",
    "            # print(post['text'][:50])\n",
    "            listposts.append(tmp)\n",
    "            # print(len(listposts))\n",
    "            count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Content arrangement\n",
    "    time = [] # Publication Time\n",
    "    contents = [] # post content\n",
    "    post_urls = [] # Post url\n",
    "    img = [] # cover photo\n",
    "    likes = []\n",
    "    num_comments = [] # number of comments\n",
    "    shares = [] # number of shares\n",
    "    publisher = [] \n",
    "    reactions = [] # haha, love, sorry...etc\n",
    "\n",
    "    sz = len(listposts)\n",
    "    for i in range(sz):\n",
    "        try: \n",
    "            post = listposts[i]\n",
    "            # Publication Time\n",
    "            time.append(post['time'])\n",
    "            # post content\n",
    "            tmp_ = post['post_text'].split(' 個讚')[0].split('\\n')[:-1]\n",
    "            contents_prim = '，'.join(tmp_)\n",
    "            contents.append(contents_prim)\n",
    "            # Post url\n",
    "            post_urls.append(post['post_url'])\n",
    "            # cover photo\n",
    "            img.append(post['image_lowquality'])\n",
    "            # likes\n",
    "            likes.append(post['likes'])\n",
    "            # number of comments\n",
    "            num_comments.append(post['comments'])\n",
    "            # number of shares\n",
    "            shares.append(post['shares'])\n",
    "            # publisher\n",
    "            publisher.append(post['username'].split('\\n')[0])\n",
    "            # Reactions,ex. haha, love, sorry...etc\n",
    "            reactions.append(post['reactions'])\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    # Make DataFrame\n",
    "    post_df = pd.DataFrame({\n",
    "        'Publisher': publisher,\n",
    "        'Publication_Time': time,\n",
    "        'Post_Content': contents,\n",
    "        'Post_url': post_urls,\n",
    "        'Likes': likes,\n",
    "        'Reactions': reactions,\n",
    "        'Comments': num_comments,\n",
    "        'Shares': shares,\n",
    "        'Cover_photo': img\n",
    "    })\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb680ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current domain: pnnpts\n",
      "Current Process: 1/2...\n",
      "Current domain: twreporter\n",
      "Current Process: 1/2...\n",
      "Current domain: myudn\n",
      "Current Process: 1/2...\n"
     ]
    }
   ],
   "source": [
    "# # test result\n",
    "# fb_posts = pd.DataFrame()\n",
    "\n",
    "# for page in reporter_list: \n",
    "#     df = pd.DataFrame()\n",
    "#     df = fb_crawler(page, 2)\n",
    "#     fb_posts = pd.concat([fb_posts, df])\n",
    "    \n",
    "# fb_posts = fb_posts.reset_index(drop=True)\n",
    "# fb_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224cf8e",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4158a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reporter_list))\n",
    "print(len(finance_list))\n",
    "print(len(company_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_posts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e8dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 新聞網\n",
    "for page in reporter_list: \n",
    "    df = fb_crawler(page, 100)\n",
    "    print(page, len(df['Publisher']))\n",
    "    fb_posts = pd.concat([fb_posts, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6aa9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 財經網站\n",
    "for page in finance_list: \n",
    "    df = fb_crawler(page, 100)\n",
    "    print(page, len(df['Publisher']))\n",
    "    fb_posts = pd.concat([fb_posts, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公司\n",
    "for page in company_list: \n",
    "    df = fb_crawler(page, 100)\n",
    "    print(page, len(df['Publisher']))\n",
    "    fb_posts = pd.concat([fb_posts, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_posts = fb_posts.reset_index(drop=True)\n",
    "fb_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/USER/Desktop/Prof_Hsieh_Project/about_emotion/Chinese_derivative/corpus'\n",
    "os.chdir(path)\n",
    "StartDate = (datetime.date.today() + datetime.timedelta(days=-7)).strftime('%Y-%m-%d')\n",
    "StartDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = StartDate + \"_Facebook_Post_Data.pkl\"\n",
    "fb_posts.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951fb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63feea71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a57858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
