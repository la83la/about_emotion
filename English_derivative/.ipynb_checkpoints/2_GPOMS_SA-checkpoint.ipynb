{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390eb1e5",
   "metadata": {},
   "source": [
    "論文懶人包：<br>\n",
    "https://github.com/aronwc/mlsm/blob/master/papers/bollen10twitter/vlandeir.md<br>\n",
    "https://zhuanlan.zhihu.com/p/19944548<br>\n",
    "使用到：<br>\n",
    "https://github.com/kbathina/sentiment-tools\n",
    "\n",
    "They create their own tool : Google-Profile Of Mood States (GPOMS), an extended version of POMS-bi. It is based on a lexicon of 964 terms and can measure human mood states in six mood dimensions : Calm, Alert, Sure, Vital, Kind and Happy.<br>\n",
    "\n",
    "1. Calm: composed/anxious\n",
    "2. Kind: agreeable/hostile\n",
    "3. Happy: elated/depressed\n",
    "4. Alert: confident/unsure\n",
    "5. Sure: clearheaded/confuse\n",
    "6. Vital: energetic/tired\n",
    "\n",
    "[詞彙表總覽]\n",
    "https://github.com/kbathina/sentiment-tools/blob/master/moodscores/data/GPOMS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d844facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9a27a",
   "metadata": {},
   "source": [
    "# I. load tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3a2987",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Related_dataset/corpus.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m condense_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRelated_dataset/corpus.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m condense_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    186\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Related_dataset/corpus.pkl'"
     ]
    }
   ],
   "source": [
    "condense_df = pd.read_pickle(\"Related_dataset/corpus.pkl\")\n",
    "condense_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6db06",
   "metadata": {},
   "source": [
    "# II. Process the word list of sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ea712",
   "metadata": {},
   "source": [
    "    try! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = 'This tool is easy and fun to use!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf85ae7",
   "metadata": {},
   "source": [
    "    pre emotion: 996 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607fedc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moodscores import gpoms\n",
    "GPOMS = gpoms.GPOMS()\n",
    "\n",
    "sentiment_gpoms_sum, tokens_gpoms_sum = GPOMS.Score(tweet,'Sum')\n",
    "sentiment_gpoms_ave, tokens_gpoms_ave = GPOMS.Score(tweet,'Average')\n",
    "\n",
    "# anxious, hostile, depressed, unsure, confused, tired\n",
    "for dimension in sentiment_gpoms_sum.keys():\n",
    "    print(dimension + ' score (Sum) =', sentiment_gpoms_sum[dimension])\n",
    "    print(dimension + ' score (Average) =', sentiment_gpoms_ave[dimension])\n",
    "\n",
    "print('Tokenized list =', tokens_gpoms_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d77299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f8bfd0d",
   "metadata": {},
   "source": [
    "# III. Count the ratio of positive v.s. negative on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc35a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = condense_df.copy()\n",
    "\n",
    "def pre(list):\n",
    "    return \" \".join(list)\n",
    "merge_df['GPOMS_preprocess_data'] = merge_df['tweets_processed'].map(pre)\n",
    "\n",
    "def sentiment_GPOMS_sum(tweet):\n",
    "    sentiment_gpoms_sum, tokens_gpoms_sum = GPOMS.Score(tweet,'Sum')\n",
    "    tmp = []\n",
    "    for dimension in sentiment_gpoms_sum.keys():\n",
    "        tmp.append(sentiment_gpoms_sum[dimension])\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def sentiment_GPOMS_avg(tweet):\n",
    "    sentiment_gpoms_ave, tokens_gpoms_ave = GPOMS.Score(tweet,'Average')\n",
    "    tmp = []\n",
    "    for dimension in sentiment_gpoms_sum.keys():\n",
    "        tmp.append(sentiment_gpoms_ave[dimension])\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "merge_df['GPOMS_sum'] = merge_df['GPOMS_preprocess_data'].map(sentiment_GPOMS_sum)\n",
    "merge_df['GPOMS_avg'] = merge_df['GPOMS_preprocess_data'].map(sentiment_GPOMS_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbdf1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(list):\n",
    "    return list.index(max(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['sum_index'] = merge_df['GPOMS_sum'].map(max_index)\n",
    "merge_df['avg_index'] = merge_df['GPOMS_avg'].map(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb00986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df.to_pickle(\"Related_dataset/GPOMS_result.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f496635",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(merge_df, columns = [\"tweet_size\", \"avg_index\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a4e3d",
   "metadata": {},
   "source": [
    "### 加入絕對值再去做比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS = merge_df.copy()\n",
    "    \n",
    "GPOMS = GPOMS.drop([\"tweets_processed\", \"tweet_size\", \"GPOMS_preprocess_data\", \"sum_index\", \"avg_index\"], axis=1)\n",
    "GPOMS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8bd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS_prim = GPOMS.copy()\n",
    "GPOMS_prim[['Calm','Kind','Happy','Alert','Sure','Vital']] = pd.DataFrame(GPOMS.GPOMS_avg.tolist(), index= GPOMS.index)\n",
    "GPOMS_prim = GPOMS_prim.drop(['GPOMS_sum', 'GPOMS_avg'], axis=1)\n",
    "GPOMS_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPOMS_prim['Calm_pvn'] = (GPOMS_prim['Calm']>0)\n",
    "# GPOMS_prim['Kind_pvn'] = (GPOMS_prim['Kind']>0)\n",
    "# GPOMS_prim['Happy_pvn'] = (GPOMS_prim['Happy']>0)\n",
    "# GPOMS_prim['Alert_pvn'] = (GPOMS_prim['Alert']>0)\n",
    "# GPOMS_prim['Sure_pvn'] = (GPOMS_prim['Sure']>0)\n",
    "# GPOMS_prim['Vital_pvn'] = (GPOMS_prim['Vital']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764dd3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GPOMS_prim['Max_value'] = GPOMS_prim.apply(lambda x: max(x.min(), x.max(), key=abs),axis=1)\n",
    "GPOMS_prim['Max_type'] = GPOMS_prim.idxmax(axis=1)\n",
    "GPOMS_prim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9812158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GPOMS_prim['Max_type'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS_prim['Max_value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = list(GPOMS_prim['Max_type'])\n",
    "dict((a, tmp.count(a)) for a in tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3904b86",
   "metadata": {},
   "source": [
    "很奇怪，77天都是Alert最高，這個資料好像不應該有這個特性?? (他不是企業推文ㄌ)- wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新資料有好幾天都沒有推文資料，不是連續ㄉ:0\n",
    "tmp = GPOMS_prim.copy()\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "my_range = pd.date_range(\n",
    "  start=\"2020-04-09\", end=\"2020-07-16\", freq='B')\n",
    " \n",
    "print(my_range.difference(tmp['date_prune']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa51d64",
   "metadata": {},
   "source": [
    "## IV. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatization(sentence):\n",
    "    tokens = word_tokenize(sentence)  # 分詞\n",
    "    tagged_sent = pos_tag(tokens)     # 獲取單詞詞性\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 詞形還原\n",
    "\n",
    "    return lemmas_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced51025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑很久，到這邊我就沒繼續了，你可以再看看找不找的到原因- wa\n",
    "\n",
    "def pre(list):\n",
    "    return \" \".join(list)\n",
    "\n",
    "def sentiment_GPOMS_sum(tweet):\n",
    "    sentiment_gpoms_sum, tokens_gpoms_sum = GPOMS.Score(tweet,'Sum')\n",
    "    tmp = []\n",
    "    for dimension in sentiment_gpoms_sum.keys():\n",
    "        tmp.append(sentiment_gpoms_sum[dimension])\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def sentiment_GPOMS_avg(tweet):\n",
    "    sentiment_gpoms_ave, tokens_gpoms_ave = GPOMS.Score(tweet,'Average')\n",
    "    tmp = []\n",
    "    for dimension in sentiment_gpoms_sum.keys():\n",
    "        tmp.append(sentiment_gpoms_ave[dimension])\n",
    "    return tmp\n",
    "\n",
    "merge_df = condense_df.copy()\n",
    "merge_df['GPOMS_preprocess_data'] = merge_df['tweets_processed'].map(pre)\n",
    "del merge_df['tweets_processed']\n",
    "\n",
    "merge_df['lemmatization'] = merge_df['GPOMS_preprocess_data'].map(lemmatization)\n",
    "del merge_df['GPOMS_preprocess_data']\n",
    "\n",
    "merge_df['GPOMS_preprocess_data'] = merge_df['lemmatization'].map(pre)\n",
    "del merge_df['lemmatization']\n",
    "del merge_df['tweet_size']\n",
    "\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['GPOMS_sum_lemmatized'] = merge_df['GPOMS_preprocess_data'].map(sentiment_GPOMS_sum)\n",
    "merge_df['GPOMS_avg_lemmatized'] = merge_df['GPOMS_preprocess_data'].map(sentiment_GPOMS_avg)\n",
    "\n",
    "merge_df = merge_df.drop([\"GPOMS_preprocess_data\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7715b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS = merge_df\n",
    "GPOMS_prim = merge_df\n",
    "GPOMS_prim[['Calm','Kind','Happy','Alert','Sure','Vital']] = pd.DataFrame(GPOMS.GPOMS_avg_lemmatized.tolist(), index= GPOMS.index)\n",
    "GPOMS_prim = GPOMS_prim.drop(['GPOMS_sum_lemmatized', 'GPOMS_avg_lemmatized'], axis=1)\n",
    "GPOMS_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS_prim['Max_value'] = GPOMS_prim.apply(lambda x: max(x.min(), x.max(), key=abs),axis=1)\n",
    "GPOMS_prim['Max_type'] = GPOMS_prim.idxmax(axis=1)\n",
    "GPOMS_prim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01030451",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOMS_prim['Max_value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279443e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = list(GPOMS_prim['Max_type'])\n",
    "dict((a, tmp.count(a)) for a in tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
