{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a2fbda",
   "metadata": {},
   "source": [
    "# 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9672ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Using cached tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/anaconda3/lib/python3.7/site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.0 requests-oauthlib-1.3.1 tweepy-4.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0058dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy  # https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "import hashlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff38a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# CREDENTIALS\n",
    "api_key = \"zGWacA8CkRVNQgY5vgAh4nfAp\"\n",
    "api_secret_key = \"DWaRZpAls3u4YcotLKcImKeYy03qlcBqXBKlDr6CtK5Y33Jc3I\"\n",
    "access_token = \"1140211942436429825-5D5hkMjOGr5WyxciM8qO7WH6lz0cOq\"\n",
    "access_token_secret = \"AZZ0OhhV2t8W1n9Q8MqopgtvZaChXsZzxwr1CbF9OmcIz\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d30377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 33 companied of DJIA\n",
    "companies = \"3M,Walgreens,Salesforce,Amgen,Honeywell,American Express,Apple,Boeing,Caterpillar,Chevron,Cisco Systems,\" \\\n",
    "            \"Coca-Cola,ExxonMobil,General Electric,Goldman Sachs,IBM,\" \\\n",
    "            \"Intel,Johnson & Johnson,JPMorgan Chase,McDonald's,Merck,Microsoft,Nike,\" \\\n",
    "            \"Pfizer,Procter & Gamble,The Home Depot,Travelers,United Technologies,\" \\\n",
    "            \"UnitedHealth Group,Verizon,Visa,Walmart,Walt Disney\".split(\",\")\n",
    "\n",
    "accounts = \"3M,Walgreens,Salesforce,Amgen,Honeywell,AmericanExpress,AppleSupport,Boeing,CaterpillarInc,Chevron,Cisco,\" \\\n",
    "           \"CocaCola,exxonmobil,generalelectric,GoldmanSachs,IBM,intel,\" \\\n",
    "           \"JNJNews,jpmorgan,McDonalds,Merck,Microsoft,Nike,pfizer,ProcterGamble,\" \\\n",
    "           \"HomeDepot,Travelers,UTC,UnitedHealthGrp,verizon,Visa,Walmart,DisneyStudios\".split(\",\")\n",
    "\n",
    "comDic = dict(zip(accounts, companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba40b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3M': '3M',\n",
       " 'Walgreens': 'Walgreens',\n",
       " 'Salesforce': 'Salesforce',\n",
       " 'Amgen': 'Amgen',\n",
       " 'Honeywell': 'Honeywell',\n",
       " 'AmericanExpress': 'American Express',\n",
       " 'AppleSupport': 'Apple',\n",
       " 'Boeing': 'Boeing',\n",
       " 'CaterpillarInc': 'Caterpillar',\n",
       " 'Chevron': 'Chevron',\n",
       " 'Cisco': 'Cisco Systems',\n",
       " 'CocaCola': 'Coca-Cola',\n",
       " 'exxonmobil': 'ExxonMobil',\n",
       " 'generalelectric': 'General Electric',\n",
       " 'GoldmanSachs': 'Goldman Sachs',\n",
       " 'IBM': 'IBM',\n",
       " 'intel': 'Intel',\n",
       " 'JNJNews': 'Johnson & Johnson',\n",
       " 'jpmorgan': 'JPMorgan Chase',\n",
       " 'McDonalds': \"McDonald's\",\n",
       " 'Merck': 'Merck',\n",
       " 'Microsoft': 'Microsoft',\n",
       " 'Nike': 'Nike',\n",
       " 'pfizer': 'Pfizer',\n",
       " 'ProcterGamble': 'Procter & Gamble',\n",
       " 'HomeDepot': 'The Home Depot',\n",
       " 'Travelers': 'Travelers',\n",
       " 'UTC': 'United Technologies',\n",
       " 'UnitedHealthGrp': 'UnitedHealth Group',\n",
       " 'verizon': 'Verizon',\n",
       " 'Visa': 'Visa',\n",
       " 'Walmart': 'Walmart',\n",
       " 'DisneyStudios': 'Walt Disney'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746f6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the tweets base on their screen_name(user_id)\n",
    "def get_all_tweets(screen_name):\n",
    "    # Twitter only allows access to a users most recent 3200 tweets with this method\n",
    "\n",
    "    print(\"collecting tweets from: \" + screen_name)\n",
    "    # initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    # keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        # all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=200,\n",
    "                                       max_id=oldest, exclude_replies=True)\n",
    "\n",
    "        # save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        # update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    # transform the tweepy tweets into a 2D array that will populate the csv\n",
    "    outtweets = [[\n",
    "        tweet.created_at,\n",
    "        comDic[screen_name],\n",
    "        \"https://twitter.com/\" + screen_name + \"/status/\" + tweet.id_str,\n",
    "        tweet.text.encode(\"utf-8\"),\n",
    "        tweet.retweet_count,\n",
    "        tweet.favorite_count,\n",
    "        tweet.id_str]\n",
    "        for tweet in alltweets]\n",
    "\n",
    "    # compose dataframe for outputting\n",
    "    df = pd.DataFrame(columns=[\"created_at\", \"company\", \"url\", \"tweets\", \"re_tweets\", \"likes\", \"id\"])\n",
    "    for tweet in outtweets:\n",
    "        row = pd.Series(tweet,index=[\"created_at\", \"company\", \"url\", \"tweets\", \"re_tweets\", \"likes\", \"id\"])\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fd7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting tweets from: 3M\n",
      "...254 tweets downloaded so far\n",
      "...294 tweets downloaded so far\n",
      "...355 tweets downloaded so far\n",
      "...408 tweets downloaded so far\n",
      "...467 tweets downloaded so far\n",
      "...534 tweets downloaded so far\n",
      "...576 tweets downloaded so far\n",
      "...622 tweets downloaded so far\n",
      "...678 tweets downloaded so far\n",
      "...727 tweets downloaded so far\n",
      "...749 tweets downloaded so far\n",
      "...766 tweets downloaded so far\n",
      "...799 tweets downloaded so far\n",
      "...831 tweets downloaded so far\n",
      "...863 tweets downloaded so far\n",
      "...893 tweets downloaded so far\n",
      "...893 tweets downloaded so far\n",
      "collecting tweets from: Walgreens\n",
      "...206 tweets downloaded so far\n",
      "...207 tweets downloaded so far\n",
      "...209 tweets downloaded so far\n",
      "...210 tweets downloaded so far\n",
      "...211 tweets downloaded so far\n",
      "...212 tweets downloaded so far\n",
      "...214 tweets downloaded so far\n",
      "...225 tweets downloaded so far\n",
      "...229 tweets downloaded so far\n",
      "...236 tweets downloaded so far\n",
      "...246 tweets downloaded so far\n",
      "...250 tweets downloaded so far\n",
      "...257 tweets downloaded so far\n",
      "...261 tweets downloaded so far\n",
      "...264 tweets downloaded so far\n",
      "...264 tweets downloaded so far\n",
      "collecting tweets from: Salesforce\n",
      "...253 tweets downloaded so far\n",
      "...333 tweets downloaded so far\n",
      "...437 tweets downloaded so far\n",
      "...578 tweets downloaded so far\n",
      "...627 tweets downloaded so far\n",
      "...752 tweets downloaded so far\n",
      "...861 tweets downloaded so far\n",
      "...922 tweets downloaded so far\n",
      "...939 tweets downloaded so far\n",
      "...985 tweets downloaded so far\n",
      "...1059 tweets downloaded so far\n",
      "...1060 tweets downloaded so far\n",
      "...1060 tweets downloaded so far\n",
      "collecting tweets from: Amgen\n",
      "...383 tweets downloaded so far\n",
      "...545 tweets downloaded so far\n",
      "...736 tweets downloaded so far\n",
      "...933 tweets downloaded so far\n",
      "...1127 tweets downloaded so far\n",
      "...1313 tweets downloaded so far\n",
      "...1507 tweets downloaded so far\n",
      "...1667 tweets downloaded so far\n",
      "...1834 tweets downloaded so far\n",
      "...1975 tweets downloaded so far\n",
      "...2148 tweets downloaded so far\n",
      "...2286 tweets downloaded so far\n",
      "...2464 tweets downloaded so far\n",
      "...2649 tweets downloaded so far\n",
      "...2822 tweets downloaded so far\n",
      "...2868 tweets downloaded so far\n",
      "...2868 tweets downloaded so far\n",
      "collecting tweets from: Honeywell\n",
      "...251 tweets downloaded so far\n",
      "...313 tweets downloaded so far\n",
      "...362 tweets downloaded so far\n",
      "...420 tweets downloaded so far\n",
      "...463 tweets downloaded so far\n",
      "...495 tweets downloaded so far\n",
      "...530 tweets downloaded so far\n",
      "...573 tweets downloaded so far\n",
      "...606 tweets downloaded so far\n",
      "...629 tweets downloaded so far\n",
      "...686 tweets downloaded so far\n",
      "...757 tweets downloaded so far\n",
      "...871 tweets downloaded so far\n",
      "...990 tweets downloaded so far\n",
      "...1176 tweets downloaded so far\n",
      "...1268 tweets downloaded so far\n",
      "...1268 tweets downloaded so far\n",
      "collecting tweets from: AmericanExpress\n",
      "...224 tweets downloaded so far\n",
      "...253 tweets downloaded so far\n",
      "...284 tweets downloaded so far\n",
      "...302 tweets downloaded so far\n",
      "...353 tweets downloaded so far\n",
      "...408 tweets downloaded so far\n",
      "...436 tweets downloaded so far\n",
      "...485 tweets downloaded so far\n",
      "...510 tweets downloaded so far\n",
      "...563 tweets downloaded so far\n",
      "...598 tweets downloaded so far\n",
      "...637 tweets downloaded so far\n",
      "...676 tweets downloaded so far\n",
      "...700 tweets downloaded so far\n",
      "...759 tweets downloaded so far\n",
      "...788 tweets downloaded so far\n",
      "...788 tweets downloaded so far\n",
      "collecting tweets from: AppleSupport\n",
      "...201 tweets downloaded so far\n",
      "...202 tweets downloaded so far\n",
      "...203 tweets downloaded so far\n",
      "...204 tweets downloaded so far\n",
      "...205 tweets downloaded so far\n",
      "...206 tweets downloaded so far\n",
      "...207 tweets downloaded so far\n",
      "...208 tweets downloaded so far\n",
      "...209 tweets downloaded so far\n",
      "...210 tweets downloaded so far\n",
      "...212 tweets downloaded so far\n",
      "...212 tweets downloaded so far\n",
      "collecting tweets from: Boeing\n",
      "...363 tweets downloaded so far\n",
      "...496 tweets downloaded so far\n",
      "...683 tweets downloaded so far\n",
      "...862 tweets downloaded so far\n",
      "...1053 tweets downloaded so far\n",
      "...1233 tweets downloaded so far\n",
      "...1346 tweets downloaded so far\n",
      "...1512 tweets downloaded so far\n",
      "...1710 tweets downloaded so far\n",
      "...1899 tweets downloaded so far\n",
      "...2092 tweets downloaded so far\n",
      "...2292 tweets downloaded so far\n",
      "...2490 tweets downloaded so far\n",
      "...2675 tweets downloaded so far\n",
      "...2862 tweets downloaded so far\n",
      "...2932 tweets downloaded so far\n",
      "...2932 tweets downloaded so far\n",
      "collecting tweets from: CaterpillarInc\n",
      "...315 tweets downloaded so far\n",
      "...460 tweets downloaded so far\n",
      "...591 tweets downloaded so far\n",
      "...734 tweets downloaded so far\n",
      "...885 tweets downloaded so far\n",
      "...955 tweets downloaded so far\n",
      "...1013 tweets downloaded so far\n",
      "...1145 tweets downloaded so far\n",
      "...1299 tweets downloaded so far\n",
      "...1455 tweets downloaded so far\n",
      "...1608 tweets downloaded so far\n",
      "...1733 tweets downloaded so far\n",
      "...1846 tweets downloaded so far\n",
      "...1977 tweets downloaded so far\n",
      "...2140 tweets downloaded so far\n",
      "...2190 tweets downloaded so far\n",
      "...2190 tweets downloaded so far\n",
      "collecting tweets from: Chevron\n",
      "...279 tweets downloaded so far\n",
      "...334 tweets downloaded so far\n",
      "...379 tweets downloaded so far\n",
      "...429 tweets downloaded so far\n",
      "...461 tweets downloaded so far\n",
      "...509 tweets downloaded so far\n",
      "...564 tweets downloaded so far\n",
      "...630 tweets downloaded so far\n",
      "...718 tweets downloaded so far\n",
      "...811 tweets downloaded so far\n",
      "...881 tweets downloaded so far\n",
      "...955 tweets downloaded so far\n",
      "...1043 tweets downloaded so far\n",
      "...1102 tweets downloaded so far\n",
      "...1158 tweets downloaded so far\n",
      "...1194 tweets downloaded so far\n",
      "...1194 tweets downloaded so far\n",
      "collecting tweets from: Cisco\n",
      "...376 tweets downloaded so far\n",
      "...551 tweets downloaded so far\n",
      "...734 tweets downloaded so far\n",
      "...895 tweets downloaded so far\n",
      "...1045 tweets downloaded so far\n",
      "...1196 tweets downloaded so far\n",
      "...1376 tweets downloaded so far\n",
      "...1543 tweets downloaded so far\n",
      "...1717 tweets downloaded so far\n",
      "...1887 tweets downloaded so far\n",
      "...2053 tweets downloaded so far\n",
      "...2190 tweets downloaded so far\n",
      "...2322 tweets downloaded so far\n",
      "...2487 tweets downloaded so far\n",
      "...2668 tweets downloaded so far\n",
      "...2720 tweets downloaded so far\n",
      "...2720 tweets downloaded so far\n",
      "collecting tweets from: CocaCola\n",
      "...205 tweets downloaded so far\n",
      "...220 tweets downloaded so far\n",
      "...235 tweets downloaded so far\n",
      "...245 tweets downloaded so far\n",
      "...256 tweets downloaded so far\n",
      "...263 tweets downloaded so far\n",
      "...269 tweets downloaded so far\n",
      "...271 tweets downloaded so far\n",
      "...278 tweets downloaded so far\n",
      "...281 tweets downloaded so far\n",
      "...286 tweets downloaded so far\n",
      "...299 tweets downloaded so far\n",
      "...302 tweets downloaded so far\n",
      "...303 tweets downloaded so far\n",
      "...308 tweets downloaded so far\n",
      "...321 tweets downloaded so far\n",
      "...321 tweets downloaded so far\n",
      "collecting tweets from: exxonmobil\n",
      "...387 tweets downloaded so far\n",
      "...586 tweets downloaded so far\n",
      "...774 tweets downloaded so far\n",
      "...962 tweets downloaded so far\n",
      "...1142 tweets downloaded so far\n",
      "...1296 tweets downloaded so far\n",
      "...1417 tweets downloaded so far\n",
      "...1510 tweets downloaded so far\n",
      "...1541 tweets downloaded so far\n",
      "...1602 tweets downloaded so far\n",
      "...1725 tweets downloaded so far\n",
      "...1923 tweets downloaded so far\n",
      "...2109 tweets downloaded so far\n",
      "...2289 tweets downloaded so far\n",
      "...2468 tweets downloaded so far\n",
      "...2530 tweets downloaded so far\n",
      "...2530 tweets downloaded so far\n",
      "collecting tweets from: generalelectric\n",
      "...364 tweets downloaded so far\n",
      "...540 tweets downloaded so far\n",
      "...708 tweets downloaded so far\n",
      "...849 tweets downloaded so far\n",
      "...1008 tweets downloaded so far\n",
      "...1191 tweets downloaded so far\n",
      "...1352 tweets downloaded so far\n",
      "...1497 tweets downloaded so far\n",
      "...1626 tweets downloaded so far\n",
      "...1746 tweets downloaded so far\n",
      "...1839 tweets downloaded so far\n",
      "...1948 tweets downloaded so far\n",
      "...2011 tweets downloaded so far\n",
      "...2056 tweets downloaded so far\n",
      "...2109 tweets downloaded so far\n",
      "...2136 tweets downloaded so far\n",
      "...2136 tweets downloaded so far\n",
      "collecting tweets from: GoldmanSachs\n",
      "...358 tweets downloaded so far\n",
      "...544 tweets downloaded so far\n",
      "...735 tweets downloaded so far\n",
      "...925 tweets downloaded so far\n",
      "...1112 tweets downloaded so far\n",
      "...1312 tweets downloaded so far\n",
      "...1511 tweets downloaded so far\n",
      "...1711 tweets downloaded so far\n",
      "...1911 tweets downloaded so far\n",
      "...2111 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...2310 tweets downloaded so far\n",
      "...2510 tweets downloaded so far\n",
      "...2710 tweets downloaded so far\n",
      "...2910 tweets downloaded so far\n",
      "...3110 tweets downloaded so far\n",
      "...3160 tweets downloaded so far\n",
      "...3160 tweets downloaded so far\n",
      "collecting tweets from: IBM\n",
      "...217 tweets downloaded so far\n",
      "...231 tweets downloaded so far\n",
      "...254 tweets downloaded so far\n",
      "...278 tweets downloaded so far\n",
      "...290 tweets downloaded so far\n",
      "...295 tweets downloaded so far\n",
      "...304 tweets downloaded so far\n",
      "...319 tweets downloaded so far\n",
      "...328 tweets downloaded so far\n",
      "...336 tweets downloaded so far\n",
      "...347 tweets downloaded so far\n",
      "...358 tweets downloaded so far\n",
      "...371 tweets downloaded so far\n",
      "...381 tweets downloaded so far\n",
      "...396 tweets downloaded so far\n",
      "...407 tweets downloaded so far\n",
      "...407 tweets downloaded so far\n",
      "collecting tweets from: intel\n",
      "...338 tweets downloaded so far\n",
      "...441 tweets downloaded so far\n",
      "...523 tweets downloaded so far\n",
      "...573 tweets downloaded so far\n",
      "...637 tweets downloaded so far\n",
      "...699 tweets downloaded so far\n",
      "...738 tweets downloaded so far\n",
      "...784 tweets downloaded so far\n",
      "...819 tweets downloaded so far\n",
      "...884 tweets downloaded so far\n",
      "...957 tweets downloaded so far\n",
      "...1044 tweets downloaded so far\n",
      "...1115 tweets downloaded so far\n",
      "...1220 tweets downloaded so far\n",
      "...1348 tweets downloaded so far\n",
      "...1402 tweets downloaded so far\n",
      "...1402 tweets downloaded so far\n",
      "collecting tweets from: JNJNews\n",
      "...242 tweets downloaded so far\n",
      "...327 tweets downloaded so far\n",
      "...357 tweets downloaded so far\n",
      "...411 tweets downloaded so far\n",
      "...483 tweets downloaded so far\n",
      "...634 tweets downloaded so far\n",
      "...790 tweets downloaded so far\n",
      "...946 tweets downloaded so far\n",
      "...1081 tweets downloaded so far\n",
      "...1256 tweets downloaded so far\n",
      "...1414 tweets downloaded so far\n",
      "...1557 tweets downloaded so far\n",
      "...1710 tweets downloaded so far\n",
      "...1860 tweets downloaded so far\n",
      "...1972 tweets downloaded so far\n",
      "...1977 tweets downloaded so far\n",
      "...1977 tweets downloaded so far\n",
      "collecting tweets from: jpmorgan\n",
      "...362 tweets downloaded so far\n",
      "...502 tweets downloaded so far\n",
      "...658 tweets downloaded so far\n",
      "...825 tweets downloaded so far\n",
      "...1017 tweets downloaded so far\n",
      "...1208 tweets downloaded so far\n",
      "...1401 tweets downloaded so far\n",
      "...1596 tweets downloaded so far\n",
      "...1787 tweets downloaded so far\n",
      "...1977 tweets downloaded so far\n",
      "...2167 tweets downloaded so far\n",
      "...2366 tweets downloaded so far\n",
      "...2565 tweets downloaded so far\n",
      "...2742 tweets downloaded so far\n",
      "...2938 tweets downloaded so far\n",
      "...2996 tweets downloaded so far\n",
      "...2996 tweets downloaded so far\n",
      "collecting tweets from: McDonalds\n",
      "...201 tweets downloaded so far\n",
      "...201 tweets downloaded so far\n",
      "collecting tweets from: Merck\n",
      "...391 tweets downloaded so far\n",
      "...587 tweets downloaded so far\n",
      "...759 tweets downloaded so far\n",
      "...932 tweets downloaded so far\n",
      "...1074 tweets downloaded so far\n",
      "...1077 tweets downloaded so far\n",
      "...1189 tweets downloaded so far\n",
      "...1383 tweets downloaded so far\n",
      "...1573 tweets downloaded so far\n",
      "...1711 tweets downloaded so far\n",
      "...1868 tweets downloaded so far\n",
      "...2055 tweets downloaded so far\n",
      "...2233 tweets downloaded so far\n",
      "...2420 tweets downloaded so far\n",
      "...2602 tweets downloaded so far\n",
      "...2746 tweets downloaded so far\n",
      "...2746 tweets downloaded so far\n",
      "collecting tweets from: Microsoft\n",
      "...248 tweets downloaded so far\n",
      "...318 tweets downloaded so far\n",
      "...369 tweets downloaded so far\n",
      "...423 tweets downloaded so far\n",
      "...468 tweets downloaded so far\n",
      "...512 tweets downloaded so far\n",
      "...562 tweets downloaded so far\n",
      "...614 tweets downloaded so far\n",
      "...672 tweets downloaded so far\n",
      "...728 tweets downloaded so far\n",
      "...777 tweets downloaded so far\n",
      "...825 tweets downloaded so far\n",
      "...870 tweets downloaded so far\n",
      "...917 tweets downloaded so far\n",
      "...953 tweets downloaded so far\n",
      "...990 tweets downloaded so far\n",
      "...990 tweets downloaded so far\n",
      "collecting tweets from: Nike\n",
      "...223 tweets downloaded so far\n",
      "...233 tweets downloaded so far\n",
      "...251 tweets downloaded so far\n",
      "...257 tweets downloaded so far\n",
      "...264 tweets downloaded so far\n",
      "...277 tweets downloaded so far\n",
      "...282 tweets downloaded so far\n",
      "...290 tweets downloaded so far\n",
      "...293 tweets downloaded so far\n",
      "...297 tweets downloaded so far\n",
      "...304 tweets downloaded so far\n",
      "...306 tweets downloaded so far\n",
      "...308 tweets downloaded so far\n",
      "...314 tweets downloaded so far\n",
      "...317 tweets downloaded so far\n",
      "...318 tweets downloaded so far\n",
      "...318 tweets downloaded so far\n",
      "collecting tweets from: pfizer\n",
      "...351 tweets downloaded so far\n",
      "...522 tweets downloaded so far\n",
      "...677 tweets downloaded so far\n",
      "...839 tweets downloaded so far\n",
      "...995 tweets downloaded so far\n",
      "...1167 tweets downloaded so far\n",
      "...1324 tweets downloaded so far\n",
      "...1474 tweets downloaded so far\n",
      "...1661 tweets downloaded so far\n",
      "...1850 tweets downloaded so far\n",
      "...2030 tweets downloaded so far\n",
      "...2204 tweets downloaded so far\n",
      "...2393 tweets downloaded so far\n",
      "...2586 tweets downloaded so far\n",
      "...2778 tweets downloaded so far\n",
      "...2828 tweets downloaded so far\n",
      "...2828 tweets downloaded so far\n",
      "collecting tweets from: ProcterGamble\n",
      "...258 tweets downloaded so far\n",
      "...347 tweets downloaded so far\n",
      "...375 tweets downloaded so far\n",
      "...405 tweets downloaded so far\n",
      "...469 tweets downloaded so far\n",
      "...549 tweets downloaded so far\n",
      "...625 tweets downloaded so far\n",
      "...702 tweets downloaded so far\n",
      "...783 tweets downloaded so far\n",
      "...860 tweets downloaded so far\n",
      "...934 tweets downloaded so far\n",
      "...994 tweets downloaded so far\n",
      "...1062 tweets downloaded so far\n",
      "...1124 tweets downloaded so far\n",
      "...1204 tweets downloaded so far\n",
      "...1231 tweets downloaded so far\n",
      "...1231 tweets downloaded so far\n",
      "collecting tweets from: HomeDepot\n",
      "...365 tweets downloaded so far\n",
      "...504 tweets downloaded so far\n",
      "...679 tweets downloaded so far\n",
      "...838 tweets downloaded so far\n",
      "...1001 tweets downloaded so far\n",
      "...1106 tweets downloaded so far\n",
      "...1225 tweets downloaded so far\n",
      "...1311 tweets downloaded so far\n",
      "...1506 tweets downloaded so far\n",
      "...1690 tweets downloaded so far\n",
      "...1883 tweets downloaded so far\n",
      "...2081 tweets downloaded so far\n",
      "...2279 tweets downloaded so far\n",
      "...2474 tweets downloaded so far\n",
      "...2668 tweets downloaded so far\n",
      "...2726 tweets downloaded so far\n",
      "...2726 tweets downloaded so far\n",
      "collecting tweets from: Travelers\n",
      "...357 tweets downloaded so far\n",
      "...505 tweets downloaded so far\n",
      "...642 tweets downloaded so far\n",
      "...781 tweets downloaded so far\n",
      "...921 tweets downloaded so far\n",
      "...984 tweets downloaded so far\n",
      "...1128 tweets downloaded so far\n",
      "...1277 tweets downloaded so far\n",
      "...1381 tweets downloaded so far\n",
      "...1540 tweets downloaded so far\n",
      "...1709 tweets downloaded so far\n",
      "...1867 tweets downloaded so far\n",
      "...2026 tweets downloaded so far\n",
      "...2174 tweets downloaded so far\n",
      "...2327 tweets downloaded so far\n",
      "...2373 tweets downloaded so far\n",
      "...2373 tweets downloaded so far\n",
      "collecting tweets from: UTC\n",
      "...384 tweets downloaded so far\n",
      "...567 tweets downloaded so far\n",
      "...741 tweets downloaded so far\n",
      "...908 tweets downloaded so far\n",
      "...1094 tweets downloaded so far\n",
      "...1284 tweets downloaded so far\n",
      "...1471 tweets downloaded so far\n",
      "...1650 tweets downloaded so far\n",
      "...1837 tweets downloaded so far\n",
      "...2031 tweets downloaded so far\n",
      "...2213 tweets downloaded so far\n",
      "...2397 tweets downloaded so far\n",
      "...2575 tweets downloaded so far\n",
      "...2768 tweets downloaded so far\n",
      "...2960 tweets downloaded so far\n",
      "...2962 tweets downloaded so far\n",
      "...2962 tweets downloaded so far\n",
      "collecting tweets from: UnitedHealthGrp\n",
      "...324 tweets downloaded so far\n",
      "...431 tweets downloaded so far\n",
      "...585 tweets downloaded so far\n",
      "...729 tweets downloaded so far\n",
      "...924 tweets downloaded so far\n",
      "...1108 tweets downloaded so far\n",
      "...1303 tweets downloaded so far\n",
      "...1502 tweets downloaded so far\n",
      "...1701 tweets downloaded so far\n",
      "...1898 tweets downloaded so far\n",
      "...2097 tweets downloaded so far\n",
      "...2297 tweets downloaded so far\n",
      "...2497 tweets downloaded so far\n",
      "...2695 tweets downloaded so far\n",
      "...2761 tweets downloaded so far\n",
      "...2761 tweets downloaded so far\n",
      "collecting tweets from: verizon\n",
      "...309 tweets downloaded so far\n",
      "...407 tweets downloaded so far\n",
      "...473 tweets downloaded so far\n",
      "...529 tweets downloaded so far\n",
      "...595 tweets downloaded so far\n",
      "...629 tweets downloaded so far\n",
      "...654 tweets downloaded so far\n",
      "...674 tweets downloaded so far\n",
      "...729 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...745 tweets downloaded so far\n",
      "...853 tweets downloaded so far\n",
      "...948 tweets downloaded so far\n",
      "...1037 tweets downloaded so far\n",
      "...1116 tweets downloaded so far\n",
      "...1191 tweets downloaded so far\n",
      "...1232 tweets downloaded so far\n",
      "...1232 tweets downloaded so far\n",
      "collecting tweets from: Visa\n",
      "...235 tweets downloaded so far\n",
      "...253 tweets downloaded so far\n",
      "...257 tweets downloaded so far\n",
      "...258 tweets downloaded so far\n",
      "...259 tweets downloaded so far\n",
      "...261 tweets downloaded so far\n",
      "...262 tweets downloaded so far\n",
      "...263 tweets downloaded so far\n",
      "...330 tweets downloaded so far\n",
      "...330 tweets downloaded so far\n",
      "collecting tweets from: Walmart\n",
      "...227 tweets downloaded so far\n",
      "...229 tweets downloaded so far\n",
      "...231 tweets downloaded so far\n",
      "...233 tweets downloaded so far\n",
      "...234 tweets downloaded so far\n",
      "...236 tweets downloaded so far\n",
      "...241 tweets downloaded so far\n",
      "...242 tweets downloaded so far\n",
      "...245 tweets downloaded so far\n",
      "...251 tweets downloaded so far\n",
      "...260 tweets downloaded so far\n",
      "...262 tweets downloaded so far\n",
      "...264 tweets downloaded so far\n",
      "...264 tweets downloaded so far\n",
      "collecting tweets from: DisneyStudios\n",
      "...366 tweets downloaded so far\n",
      "...541 tweets downloaded so far\n",
      "...725 tweets downloaded so far\n",
      "...900 tweets downloaded so far\n",
      "...1088 tweets downloaded so far\n",
      "...1275 tweets downloaded so far\n",
      "...1443 tweets downloaded so far\n",
      "...1635 tweets downloaded so far\n",
      "...1820 tweets downloaded so far\n",
      "...1997 tweets downloaded so far\n",
      "...2188 tweets downloaded so far\n",
      "...2379 tweets downloaded so far\n",
      "...2547 tweets downloaded so far\n",
      "...2732 tweets downloaded so far\n",
      "...2915 tweets downloaded so far\n",
      "...2915 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "allData_list = []\n",
    "for acc in accounts:\n",
    "    allData_list.append(get_all_tweets(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61970f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>tweets</th>\n",
       "      <th>re_tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-12 15:13:06+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1558109327424163840</td>\n",
       "      <td>b'@doddthesod Hello Simon - We are sorry to se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1558109327424163840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-11 20:49:19+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557831550422728705</td>\n",
       "      <td>b'@bleepingED Hello - Thank you for reaching o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1557831550422728705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-11 18:09:52+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557791422627123200</td>\n",
       "      <td>b'Did you know stop signs have not always been...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1557791422627123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-11 16:32:22+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557766888775720960</td>\n",
       "      <td>b'No nails? No screws? No problem!\\n\\n@command...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1557766888775720960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-11 15:03:20+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557744482048770048</td>\n",
       "      <td>b'@crazypnut Hello - Thank you for reaching ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1557744482048770048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55190</th>\n",
       "      <td>2019-04-07 19:06:00+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/11149...</td>\n",
       "      <td>b'In Dreamland, everything is possible. See #D...</td>\n",
       "      <td>104</td>\n",
       "      <td>937</td>\n",
       "      <td>1114967577883762691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55191</th>\n",
       "      <td>2019-04-07 16:00:00+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/11149...</td>\n",
       "      <td>b'Celebrate 10 years of @Disneynature in advan...</td>\n",
       "      <td>100</td>\n",
       "      <td>734</td>\n",
       "      <td>1114920771753844736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55192</th>\n",
       "      <td>2019-04-06 19:00:00+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/11146...</td>\n",
       "      <td>b'See the #1 movie in the world, in theaters n...</td>\n",
       "      <td>51</td>\n",
       "      <td>522</td>\n",
       "      <td>1114603682195095553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55193</th>\n",
       "      <td>2019-04-06 18:00:01+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/11145...</td>\n",
       "      <td>b'There\\xe2\\x80\\x99s no one quite like Steve. ...</td>\n",
       "      <td>207</td>\n",
       "      <td>1340</td>\n",
       "      <td>1114588585485918208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55194</th>\n",
       "      <td>2019-04-06 17:00:00+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/11145...</td>\n",
       "      <td>b'Have you met the newest performer? See #Dumb...</td>\n",
       "      <td>84</td>\n",
       "      <td>882</td>\n",
       "      <td>1114573481432084482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55195 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     created_at      company  \\\n",
       "0     2022-08-12 15:13:06+00:00           3M   \n",
       "1     2022-08-11 20:49:19+00:00           3M   \n",
       "2     2022-08-11 18:09:52+00:00           3M   \n",
       "3     2022-08-11 16:32:22+00:00           3M   \n",
       "4     2022-08-11 15:03:20+00:00           3M   \n",
       "...                         ...          ...   \n",
       "55190 2019-04-07 19:06:00+00:00  Walt Disney   \n",
       "55191 2019-04-07 16:00:00+00:00  Walt Disney   \n",
       "55192 2019-04-06 19:00:00+00:00  Walt Disney   \n",
       "55193 2019-04-06 18:00:01+00:00  Walt Disney   \n",
       "55194 2019-04-06 17:00:00+00:00  Walt Disney   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://twitter.com/3M/status/1558109327424163840   \n",
       "1      https://twitter.com/3M/status/1557831550422728705   \n",
       "2      https://twitter.com/3M/status/1557791422627123200   \n",
       "3      https://twitter.com/3M/status/1557766888775720960   \n",
       "4      https://twitter.com/3M/status/1557744482048770048   \n",
       "...                                                  ...   \n",
       "55190  https://twitter.com/DisneyStudios/status/11149...   \n",
       "55191  https://twitter.com/DisneyStudios/status/11149...   \n",
       "55192  https://twitter.com/DisneyStudios/status/11146...   \n",
       "55193  https://twitter.com/DisneyStudios/status/11145...   \n",
       "55194  https://twitter.com/DisneyStudios/status/11145...   \n",
       "\n",
       "                                                  tweets re_tweets likes  \\\n",
       "0      b'@doddthesod Hello Simon - We are sorry to se...         0     0   \n",
       "1      b'@bleepingED Hello - Thank you for reaching o...         0     1   \n",
       "2      b'Did you know stop signs have not always been...         2    11   \n",
       "3      b'No nails? No screws? No problem!\\n\\n@command...         2    11   \n",
       "4      b'@crazypnut Hello - Thank you for reaching ou...         0     0   \n",
       "...                                                  ...       ...   ...   \n",
       "55190  b'In Dreamland, everything is possible. See #D...       104   937   \n",
       "55191  b'Celebrate 10 years of @Disneynature in advan...       100   734   \n",
       "55192  b'See the #1 movie in the world, in theaters n...        51   522   \n",
       "55193  b'There\\xe2\\x80\\x99s no one quite like Steve. ...       207  1340   \n",
       "55194  b'Have you met the newest performer? See #Dumb...        84   882   \n",
       "\n",
       "                        id  \n",
       "0      1558109327424163840  \n",
       "1      1557831550422728705  \n",
       "2      1557791422627123200  \n",
       "3      1557766888775720960  \n",
       "4      1557744482048770048  \n",
       "...                    ...  \n",
       "55190  1114967577883762691  \n",
       "55191  1114920771753844736  \n",
       "55192  1114603682195095553  \n",
       "55193  1114588585485918208  \n",
       "55194  1114573481432084482  \n",
       "\n",
       "[55195 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData_df = pd.concat(allData_list).reset_index(drop=True)\n",
    "allData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c95d9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'@doddthesod Hello Simon - We are sorry to see this. It is not what we would expect. Please connect with our UK supp\\xe2\\x80\\xa6 https://t.co/syixQ1BNQT'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData_df['tweets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f08bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>tweets</th>\n",
       "      <th>re_tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55195</td>\n",
       "      <td>55195</td>\n",
       "      <td>55195</td>\n",
       "      <td>55195</td>\n",
       "      <td>55195.0</td>\n",
       "      <td>55195.0</td>\n",
       "      <td>55195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>54756</td>\n",
       "      <td>33</td>\n",
       "      <td>55195</td>\n",
       "      <td>55058</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>55195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2022-07-07 15:43:47+00:00</td>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>https://twitter.com/3M/status/1558109327424163840</td>\n",
       "      <td>b'Here are 5 ways Amgen supports the environme...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1558109327424163840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>3160</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5595.0</td>\n",
       "      <td>12189.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2012-05-17 12:48:23+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2022-08-14 03:05:12+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at        company  \\\n",
       "count                       55195          55195   \n",
       "unique                      54756             33   \n",
       "top     2022-07-07 15:43:47+00:00  Goldman Sachs   \n",
       "freq                            4           3160   \n",
       "first   2012-05-17 12:48:23+00:00            NaN   \n",
       "last    2022-08-14 03:05:12+00:00            NaN   \n",
       "\n",
       "                                                      url  \\\n",
       "count                                               55195   \n",
       "unique                                              55195   \n",
       "top     https://twitter.com/3M/status/1558109327424163840   \n",
       "freq                                                    1   \n",
       "first                                                 NaN   \n",
       "last                                                  NaN   \n",
       "\n",
       "                                                   tweets  re_tweets    likes  \\\n",
       "count                                               55195    55195.0  55195.0   \n",
       "unique                                              55058     1020.0   2031.0   \n",
       "top     b'Here are 5 ways Amgen supports the environme...        0.0      0.0   \n",
       "freq                                                   10     5595.0  12189.0   \n",
       "first                                                 NaN        NaN      NaN   \n",
       "last                                                  NaN        NaN      NaN   \n",
       "\n",
       "                         id  \n",
       "count                 55195  \n",
       "unique                55195  \n",
       "top     1558109327424163840  \n",
       "freq                      1  \n",
       "first                   NaN  \n",
       "last                    NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allData_df.to_pickle(\"Merge_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ae4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2022-06-01\"\n",
    "start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "end_date = \"2022-07-01\"\n",
    "end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "# Right way!\n",
    "start_date = start_date.replace(tzinfo=timezone('UTC'))\n",
    "end_date = end_date.replace(tzinfo=timezone('UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c830dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 00:00:00+00:00\n",
      "2022-07-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba776d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>tweets</th>\n",
       "      <th>re_tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-30 18:50:06+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1542581260689833986</td>\n",
       "      <td>b'Happy #SocialMediaDay \\xf0\\x9f\\x93\\xb1 Join ...</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1542581260689833986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-30 17:43:50+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1542564580563582982</td>\n",
       "      <td>b\"@sevenpiggies Hello - Thank you for messagin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1542564580563582982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-29 17:28:44+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1542198395275706369</td>\n",
       "      <td>b'@ohunt Sorry to hear of the difficulty! Plea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1542198395275706369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-29 17:01:30+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1542191540227002370</td>\n",
       "      <td>b'See how artist @gabe_gault uses artificial r...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1542191540227002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-29 16:56:50+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1542190367130402821</td>\n",
       "      <td>b'@mochesa Hello! The 3M Kenya help center can...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1542190367130402821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>2022-06-03 16:00:04+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/15327...</td>\n",
       "      <td>b'\\xf0\\x9f\\x8e\\xb6 ...all the flowers and vine...</td>\n",
       "      <td>29</td>\n",
       "      <td>153</td>\n",
       "      <td>1532753996456374274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>2022-06-03 15:00:03+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/15327...</td>\n",
       "      <td>b\"Here's to the dreamers! \\xf0\\x9f\\x92\\xab\\xf0...</td>\n",
       "      <td>28</td>\n",
       "      <td>187</td>\n",
       "      <td>1532738892004139008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>2022-06-02 19:30:41+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/15324...</td>\n",
       "      <td>b\"LA state of mind. \\xe2\\x98\\x80\\xef\\xb8\\x8f\\x...</td>\n",
       "      <td>20</td>\n",
       "      <td>168</td>\n",
       "      <td>1532444613109174272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>2022-06-02 17:00:02+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/15324...</td>\n",
       "      <td>b'We all have our own stories to tell \\xf0\\x9f...</td>\n",
       "      <td>18</td>\n",
       "      <td>162</td>\n",
       "      <td>1532406699599925266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>2022-06-01 16:00:05+00:00</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>https://twitter.com/DisneyStudios/status/15320...</td>\n",
       "      <td>b'Lights, camera, action \\xf0\\x9f\\x8e\\xac\\xe2\\...</td>\n",
       "      <td>25</td>\n",
       "      <td>169</td>\n",
       "      <td>1532029224130707457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at      company  \\\n",
       "0    2022-06-30 18:50:06+00:00           3M   \n",
       "1    2022-06-30 17:43:50+00:00           3M   \n",
       "2    2022-06-29 17:28:44+00:00           3M   \n",
       "3    2022-06-29 17:01:30+00:00           3M   \n",
       "4    2022-06-29 16:56:50+00:00           3M   \n",
       "...                        ...          ...   \n",
       "1790 2022-06-03 16:00:04+00:00  Walt Disney   \n",
       "1791 2022-06-03 15:00:03+00:00  Walt Disney   \n",
       "1792 2022-06-02 19:30:41+00:00  Walt Disney   \n",
       "1793 2022-06-02 17:00:02+00:00  Walt Disney   \n",
       "1794 2022-06-01 16:00:05+00:00  Walt Disney   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://twitter.com/3M/status/1542581260689833986   \n",
       "1     https://twitter.com/3M/status/1542564580563582982   \n",
       "2     https://twitter.com/3M/status/1542198395275706369   \n",
       "3     https://twitter.com/3M/status/1542191540227002370   \n",
       "4     https://twitter.com/3M/status/1542190367130402821   \n",
       "...                                                 ...   \n",
       "1790  https://twitter.com/DisneyStudios/status/15327...   \n",
       "1791  https://twitter.com/DisneyStudios/status/15327...   \n",
       "1792  https://twitter.com/DisneyStudios/status/15324...   \n",
       "1793  https://twitter.com/DisneyStudios/status/15324...   \n",
       "1794  https://twitter.com/DisneyStudios/status/15320...   \n",
       "\n",
       "                                                 tweets re_tweets likes  \\\n",
       "0     b'Happy #SocialMediaDay \\xf0\\x9f\\x93\\xb1 Join ...         2    16   \n",
       "1     b\"@sevenpiggies Hello - Thank you for messagin...         0     0   \n",
       "2     b'@ohunt Sorry to hear of the difficulty! Plea...         0     0   \n",
       "3     b'See how artist @gabe_gault uses artificial r...         0    15   \n",
       "4     b'@mochesa Hello! The 3M Kenya help center can...         0     1   \n",
       "...                                                 ...       ...   ...   \n",
       "1790  b'\\xf0\\x9f\\x8e\\xb6 ...all the flowers and vine...        29   153   \n",
       "1791  b\"Here's to the dreamers! \\xf0\\x9f\\x92\\xab\\xf0...        28   187   \n",
       "1792  b\"LA state of mind. \\xe2\\x98\\x80\\xef\\xb8\\x8f\\x...        20   168   \n",
       "1793  b'We all have our own stories to tell \\xf0\\x9f...        18   162   \n",
       "1794  b'Lights, camera, action \\xf0\\x9f\\x8e\\xac\\xe2\\...        25   169   \n",
       "\n",
       "                       id  \n",
       "0     1542581260689833986  \n",
       "1     1542564580563582982  \n",
       "2     1542198395275706369  \n",
       "3     1542191540227002370  \n",
       "4     1542190367130402821  \n",
       "...                   ...  \n",
       "1790  1532753996456374274  \n",
       "1791  1532738892004139008  \n",
       "1792  1532444613109174272  \n",
       "1793  1532406699599925266  \n",
       "1794  1532029224130707457  \n",
       "\n",
       "[1795 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData_df.query(' created_at >= @start_date and created_at <= @end_date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc10f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff428bcc",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef1e2a",
   "metadata": {},
   "source": [
    "## I. removal of stop-words and punctuation<br>\n",
    "First thing first. To make natural language understandable for machines.\n",
    "So here are four things to do:\n",
    "\n",
    "1. load data\n",
    "2. preprocess the sentences\n",
    "\n",
    "refer: http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35dd8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.25-cp37-cp37m-macosx_10_9_x86_64.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/lib/python3.7/site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.7.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbca1f63",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=1,\n"
     ]
    }
   ],
   "source": [
    "# 如果你想往前移再往前拿\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd06e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lala/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f31814cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EngStopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "464acc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text): \n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    \n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in EngStopWords:\n",
    "            pass #如果詞彙是個英文的停用詞的話，就略過不處理。\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74674795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'@doddthesod Hello Simon - We are sorry to see this. It is not what we would expect. Please connect with our UK supp\\xe2\\x80\\xa6 https://t.co/syixQ1BNQT'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryyy = allData_df['tweets'][0]\n",
    "# preprocess(tryyy)\n",
    "tryyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea2169f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "#start process_tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "    \n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)    \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []  \n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences \n",
    "        w = replaceTwoOrMore(w) \n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if it consists of only words\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stopWord\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector   \n",
    "\n",
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "153bb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0;\n",
    "featureList = []\n",
    "tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea260c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'@doddthesod Hello Simon - We are sorry to see this. It is not what we would expect. Please connect with our UK supp\\\\xe2\\\\x80\\\\xa6 https://t.co/syixQ1BNQT'\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73a8bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tryyy = str(tryyy)\n",
    "tweet = tryyy\n",
    "processedTweet = processTweet(tweet)\n",
    "featureVector = getFeatureVector(processedTweet, EngStopWords)\n",
    "featureList.extend(featureVector)\n",
    "tweets.append((featureVector, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7af054d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'simon', 'sorry', 'see', 'would', 'expect', 'please', 'connect', 'uk', 'url']\n",
      "[([], 'b'), (['hello', 'simon', 'sorry', 'see', 'would', 'expect', 'please', 'connect', 'uk', 'url'], 'b')]\n"
     ]
    }
   ],
   "source": [
    "print(featureList)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22f8006c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>tweets</th>\n",
       "      <th>re_tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-12 15:13:06+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1558109327424163840</td>\n",
       "      <td>b'@doddthesod Hello Simon - We are sorry to se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1558109327424163840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-11 20:49:19+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557831550422728705</td>\n",
       "      <td>b'@bleepingED Hello - Thank you for reaching o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1557831550422728705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-11 18:09:52+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557791422627123200</td>\n",
       "      <td>b'Did you know stop signs have not always been...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1557791422627123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-11 16:32:22+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557766888775720960</td>\n",
       "      <td>b'No nails? No screws? No problem!\\n\\n@command...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1557766888775720960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-11 15:03:20+00:00</td>\n",
       "      <td>3M</td>\n",
       "      <td>https://twitter.com/3M/status/1557744482048770048</td>\n",
       "      <td>b'@crazypnut Hello - Thank you for reaching ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1557744482048770048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at company  \\\n",
       "0 2022-08-12 15:13:06+00:00      3M   \n",
       "1 2022-08-11 20:49:19+00:00      3M   \n",
       "2 2022-08-11 18:09:52+00:00      3M   \n",
       "3 2022-08-11 16:32:22+00:00      3M   \n",
       "4 2022-08-11 15:03:20+00:00      3M   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://twitter.com/3M/status/1558109327424163840   \n",
       "1  https://twitter.com/3M/status/1557831550422728705   \n",
       "2  https://twitter.com/3M/status/1557791422627123200   \n",
       "3  https://twitter.com/3M/status/1557766888775720960   \n",
       "4  https://twitter.com/3M/status/1557744482048770048   \n",
       "\n",
       "                                              tweets re_tweets likes  \\\n",
       "0  b'@doddthesod Hello Simon - We are sorry to se...         0     0   \n",
       "1  b'@bleepingED Hello - Thank you for reaching o...         0     1   \n",
       "2  b'Did you know stop signs have not always been...         2    11   \n",
       "3  b'No nails? No screws? No problem!\\n\\n@command...         2    11   \n",
       "4  b'@crazypnut Hello - Thank you for reaching ou...         0     0   \n",
       "\n",
       "                    id  \n",
       "0  1558109327424163840  \n",
       "1  1557831550422728705  \n",
       "2  1557791422627123200  \n",
       "3  1557766888775720960  \n",
       "4  1557744482048770048  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "782acded",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'strftime' requires a 'datetime.date' object but received a 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tf/7633jhwn4_gfj1fldy8jf5680000gq/T/ipykernel_6763/1166150944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muse_df\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_df\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muse_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muse_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S+00:00\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muse_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0muse_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'strftime' requires a 'datetime.date' object but received a 'Series'"
     ]
    }
   ],
   "source": [
    "use_df = pd.DataFrame(allData_df, columns=['created_at','tweets'])\n",
    "# 轉換created_at\n",
    "use_df ['created_at'] = use_df ['created_at'].astype('str')\n",
    "use_df['date'] = pd.to_datetime( use_df['created_at'], format=\"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "use_df = use_df.sort_values(by=['date'], ignore_index = True)\n",
    "\n",
    "# 轉換tweets\n",
    "use_df ['tweets'] = use_df ['tweets'].astype('str')\n",
    "\n",
    "del use_df ['created_at'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc19b54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\xe2\\x80\\x9cOur customers are turning to #Cu...</td>\n",
       "      <td>2022-06-01 00:58:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'@haleydwallace Hi Haley. We would like to as...</td>\n",
       "      <td>2022-06-01 03:54:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'The latest @TalosSecurity study observes the...</td>\n",
       "      <td>2022-06-01 08:08:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'As #PrideMonth kicks off today, @masayanagis...</td>\n",
       "      <td>2022-06-01 08:48:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'The 4th Industrial Revolution has created ne...</td>\n",
       "      <td>2022-06-01 10:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>b'After getting their hands a little dirty, cr...</td>\n",
       "      <td>2022-06-30 23:00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>b\"@magius2013 @Alienware https://t.co/VYF2Vq76...</td>\n",
       "      <td>2022-06-30 23:08:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>b\"RT @Disney_Insiders: It's time to rise toget...</td>\n",
       "      <td>2022-06-30 23:26:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>b'@odieloveslattes Hi Danae, so sorry to hear ...</td>\n",
       "      <td>2022-06-30 23:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>b'RT @MicrosoftLife: \\xe2\\x80\\x9cWe have far m...</td>\n",
       "      <td>2022-06-30 23:37:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets                date\n",
       "0     b'\\xe2\\x80\\x9cOur customers are turning to #Cu... 2022-06-01 00:58:49\n",
       "1     b'@haleydwallace Hi Haley. We would like to as... 2022-06-01 03:54:07\n",
       "2     b'The latest @TalosSecurity study observes the... 2022-06-01 08:08:18\n",
       "3     b'As #PrideMonth kicks off today, @masayanagis... 2022-06-01 08:48:08\n",
       "4     b'The 4th Industrial Revolution has created ne... 2022-06-01 10:08:00\n",
       "...                                                 ...                 ...\n",
       "1790  b'After getting their hands a little dirty, cr... 2022-06-30 23:00:31\n",
       "1791  b\"@magius2013 @Alienware https://t.co/VYF2Vq76... 2022-06-30 23:08:11\n",
       "1792  b\"RT @Disney_Insiders: It's time to rise toget... 2022-06-30 23:26:35\n",
       "1793  b'@odieloveslattes Hi Danae, so sorry to hear ... 2022-06-30 23:31:00\n",
       "1794  b'RT @MicrosoftLife: \\xe2\\x80\\x9cWe have far m... 2022-06-30 23:37:39\n",
       "\n",
       "[1795 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = use_df[(use_df['date'] >= '2022-06-01') & (use_df['date'] <= '2022-07-01')]\n",
    "test = test.reset_index(drop=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d602ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tweet = str(text)\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet, EngStopWords)\n",
    "    return featureVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "25e4b51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       b'\\xe2\\x80\\x9cOur customers are turning to #Cu...\n",
       "1       b'@haleydwallace Hi Haley. We would like to as...\n",
       "2       b'The latest @TalosSecurity study observes the...\n",
       "3       b'As #PrideMonth kicks off today, @masayanagis...\n",
       "4       b'The 4th Industrial Revolution has created ne...\n",
       "                              ...                        \n",
       "1790    b'After getting their hands a little dirty, cr...\n",
       "1791    b\"@magius2013 @Alienware https://t.co/VYF2Vq76...\n",
       "1792    b\"RT @Disney_Insiders: It's time to rise toget...\n",
       "1793    b'@odieloveslattes Hi Danae, so sorry to hear ...\n",
       "1794    b'RT @MicrosoftLife: \\xe2\\x80\\x9cWe have far m...\n",
       "Name: tweets, Length: 1795, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2f0a52ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweets_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-01 00:58:49</td>\n",
       "      <td>[customers, turning, customer360, connect, cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-01 03:54:07</td>\n",
       "      <td>[hi, haley, would, like, assist, however, need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-01 08:08:18</td>\n",
       "      <td>[latest, study, observes, ongoing, campaign, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-01 08:48:08</td>\n",
       "      <td>[pridemonth, kicks, today, head, japan, prime,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-01 10:08:00</td>\n",
       "      <td>[industrial, revolution, created, new, possibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>2022-06-30 23:00:31</td>\n",
       "      <td>[getting, hands, little, dirty, creators, dyla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>2022-06-30 23:08:11</td>\n",
       "      <td>[url, july, welcome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>2022-06-30 23:26:35</td>\n",
       "      <td>[time, rise, talking, director, akin, omotoso,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>2022-06-30 23:31:00</td>\n",
       "      <td>[hi, danae, sorry, hear, looping, case, help]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>2022-06-30 23:37:39</td>\n",
       "      <td>[far, power, often, much, others, want, us, ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                   tweets_processed\n",
       "0    2022-06-01 00:58:49  [customers, turning, customer360, connect, cus...\n",
       "1    2022-06-01 03:54:07  [hi, haley, would, like, assist, however, need...\n",
       "2    2022-06-01 08:08:18  [latest, study, observes, ongoing, campaign, b...\n",
       "3    2022-06-01 08:48:08  [pridemonth, kicks, today, head, japan, prime,...\n",
       "4    2022-06-01 10:08:00  [industrial, revolution, created, new, possibi...\n",
       "...                  ...                                                ...\n",
       "1790 2022-06-30 23:00:31  [getting, hands, little, dirty, creators, dyla...\n",
       "1791 2022-06-30 23:08:11                               [url, july, welcome]\n",
       "1792 2022-06-30 23:26:35  [time, rise, talking, director, akin, omotoso,...\n",
       "1793 2022-06-30 23:31:00      [hi, danae, sorry, hear, looping, case, help]\n",
       "1794 2022-06-30 23:37:39  [far, power, often, much, others, want, us, ma...\n",
       "\n",
       "[1795 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tweets_processed'] = test.apply(lambda x: preprocess(x['tweets']),axis=1)\n",
    "del test['tweets']\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9f226",
   "metadata": {},
   "source": [
    "## II. group all tweets that were submitted on the same date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8492be38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_processed</th>\n",
       "      <th>date_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[customers, turning, customer360, connect, cus...</td>\n",
       "      <td>2022-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hi, haley, would, like, assist, however, need...</td>\n",
       "      <td>2022-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[latest, study, observes, ongoing, campaign, b...</td>\n",
       "      <td>2022-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[pridemonth, kicks, today, head, japan, prime,...</td>\n",
       "      <td>2022-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[industrial, revolution, created, new, possibi...</td>\n",
       "      <td>2022-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>[getting, hands, little, dirty, creators, dyla...</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>[url, july, welcome]</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>[time, rise, talking, director, akin, omotoso,...</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>[hi, danae, sorry, hear, looping, case, help]</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>[far, power, often, much, others, want, us, ma...</td>\n",
       "      <td>2022-06-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweets_processed date_processed\n",
       "0     [customers, turning, customer360, connect, cus...     2022-06-01\n",
       "1     [hi, haley, would, like, assist, however, need...     2022-06-01\n",
       "2     [latest, study, observes, ongoing, campaign, b...     2022-06-01\n",
       "3     [pridemonth, kicks, today, head, japan, prime,...     2022-06-01\n",
       "4     [industrial, revolution, created, new, possibi...     2022-06-01\n",
       "...                                                 ...            ...\n",
       "1790  [getting, hands, little, dirty, creators, dyla...     2022-06-30\n",
       "1791                               [url, july, welcome]     2022-06-30\n",
       "1792  [time, rise, talking, director, akin, omotoso,...     2022-06-30\n",
       "1793      [hi, danae, sorry, hear, looping, case, help]     2022-06-30\n",
       "1794  [far, power, often, much, others, want, us, ma...     2022-06-30\n",
       "\n",
       "[1795 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['date_processed'] = test.apply(lambda x: datetime.datetime.strftime(x['date'], \"%Y-%m-%d\"),axis=1)\n",
    "del test['date'] \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "22d8a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = test.groupby(\"date_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f614c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_processed\n",
       "2022-06-01     79\n",
       "2022-06-02     84\n",
       "2022-06-03     63\n",
       "2022-06-04     27\n",
       "2022-06-05     28\n",
       "2022-06-06     65\n",
       "2022-06-07     59\n",
       "2022-06-08     88\n",
       "2022-06-09     66\n",
       "2022-06-10     69\n",
       "2022-06-11     16\n",
       "2022-06-12     14\n",
       "2022-06-13    100\n",
       "2022-06-14     95\n",
       "2022-06-15    133\n",
       "2022-06-16    106\n",
       "2022-06-17     74\n",
       "2022-06-18     18\n",
       "2022-06-19     18\n",
       "2022-06-20     32\n",
       "2022-06-21     75\n",
       "2022-06-22     65\n",
       "2022-06-23    123\n",
       "2022-06-24     33\n",
       "2022-06-25     13\n",
       "2022-06-26     10\n",
       "2022-06-27     49\n",
       "2022-06-28     52\n",
       "2022-06-29     61\n",
       "2022-06-30     80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e78600b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_processed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_processed</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>[customers, turning, customer360, connect, cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>[byron, participant, program, committed, helpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>[world, physical, remote, offices, hybrid, clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-04</th>\n",
       "      <td>[digital, patient, michael, crawford, founder,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-05</th>\n",
       "      <td>[excited, present, data, asco22, oral, abstrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>[learning, improve, business, agility, stabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-07</th>\n",
       "      <td>[keep, evolving, world, hybridwork, sase, need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-08</th>\n",
       "      <td>[giving, hot, girl, coach, url, almost, needs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>[story, inspiration, years, dedication, trailb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-10</th>\n",
       "      <td>[things, need, know, sustainability, report, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-11</th>\n",
       "      <td>[hi, sorry, hear, experience, bag, please, sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-12</th>\n",
       "      <td>[pembe, founder, ceo, pema, mantar, started, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-13</th>\n",
       "      <td>[see, film, time100, url, thename, url, time10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>[first, day, bio2022, jenelle, krishnamoorthy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-15</th>\n",
       "      <td>[much, celebrate, lot, work, energizing, host,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-16</th>\n",
       "      <td>[amazing, sebas, bailarin, amoo, url, que, sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-17</th>\n",
       "      <td>[goes, building, obstacle, courses, challenge,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-18</th>\n",
       "      <td>[ways, amgen, supports, environment, cloud, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-19</th>\n",
       "      <td>[goes, building, obstacle, courses, challenge,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-20</th>\n",
       "      <td>[today, announced, entered, equity, subscripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-21</th>\n",
       "      <td>[employee, resource, organization, leads, cisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-22</th>\n",
       "      <td>[search, cancer, cure, made, way, space, innov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>[warming, legendary, night, follow, along, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>[stocks, appear, bucking, bearish, global, tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-25</th>\n",
       "      <td>[jessica, process, development, senior, scient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-26</th>\n",
       "      <td>[hi, looping, case, help, follow, final, day, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>[hola, enviarnos, un, mensaje, privado, desde,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>[ciscolive, keynote, roundup, las, ante, learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-29</th>\n",
       "      <td>[salesforcetour, nyc, chair, joined, url, prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-30</th>\n",
       "      <td>[hi, sorry, hear, something, help, relationshi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets_processed\n",
       "date_processed                                                   \n",
       "2022-06-01      [customers, turning, customer360, connect, cus...\n",
       "2022-06-02      [byron, participant, program, committed, helpi...\n",
       "2022-06-03      [world, physical, remote, offices, hybrid, clo...\n",
       "2022-06-04      [digital, patient, michael, crawford, founder,...\n",
       "2022-06-05      [excited, present, data, asco22, oral, abstrac...\n",
       "2022-06-06      [learning, improve, business, agility, stabili...\n",
       "2022-06-07      [keep, evolving, world, hybridwork, sase, need...\n",
       "2022-06-08      [giving, hot, girl, coach, url, almost, needs,...\n",
       "2022-06-09      [story, inspiration, years, dedication, trailb...\n",
       "2022-06-10      [things, need, know, sustainability, report, u...\n",
       "2022-06-11      [hi, sorry, hear, experience, bag, please, sen...\n",
       "2022-06-12      [pembe, founder, ceo, pema, mantar, started, m...\n",
       "2022-06-13      [see, film, time100, url, thename, url, time10...\n",
       "2022-06-14      [first, day, bio2022, jenelle, krishnamoorthy,...\n",
       "2022-06-15      [much, celebrate, lot, work, energizing, host,...\n",
       "2022-06-16      [amazing, sebas, bailarin, amoo, url, que, sea...\n",
       "2022-06-17      [goes, building, obstacle, courses, challenge,...\n",
       "2022-06-18      [ways, amgen, supports, environment, cloud, ma...\n",
       "2022-06-19      [goes, building, obstacle, courses, challenge,...\n",
       "2022-06-20      [today, announced, entered, equity, subscripti...\n",
       "2022-06-21      [employee, resource, organization, leads, cisc...\n",
       "2022-06-22      [search, cancer, cure, made, way, space, innov...\n",
       "2022-06-23      [warming, legendary, night, follow, along, pre...\n",
       "2022-06-24      [stocks, appear, bucking, bearish, global, tre...\n",
       "2022-06-25      [jessica, process, development, senior, scient...\n",
       "2022-06-26      [hi, looping, case, help, follow, final, day, ...\n",
       "2022-06-27      [hola, enviarnos, un, mensaje, privado, desde,...\n",
       "2022-06-28      [ciscolive, keynote, roundup, las, ante, learn...\n",
       "2022-06-29      [salesforcetour, nyc, chair, joined, url, prof...\n",
       "2022-06-30      [hi, sorry, hear, something, help, relationshi..."
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = test.groupby(\"date_processed\").agg({'tweets_processed':'sum'})\n",
    "test2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a5c61",
   "metadata": {},
   "source": [
    "## III. only take into account tweets that contain explicit statements of their author’s mood states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5444001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再塞下去怕詞太少捏～～"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c459cd",
   "metadata": {},
   "source": [
    "## IV.  filter out tweets that match the regular expressions “http:” or “www(avoid spam messages and other information-oriented tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5bb6bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已經在preprocess拿掉ㄌ！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97687c22",
   "metadata": {},
   "source": [
    "##  Another: 取得yfinance → DJIA 財金資料 to dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db3a6596",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.1.74-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests>=2.26 in /opt/anaconda3/lib/python3.7/site-packages (from yfinance) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/lib/python3.7/site-packages (from yfinance) (1.21.5)\n",
      "Collecting lxml>=4.5.1\n",
      "  Downloading lxml-4.9.1-cp37-cp37m-macosx_10_15_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting multitasking>=0.0.7\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/anaconda3/lib/python3.7/site-packages (from yfinance) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.26->yfinance) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.26->yfinance) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.16.0)\n",
      "Installing collected packages: multitasking, lxml, yfinance\n",
      "Successfully installed lxml-4.9.1 multitasking-0.0.11 yfinance-0.1.74\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd0829ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f55d47ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 6, 1, 0, 0, tzinfo=<UTC>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e57d223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(ticker):\n",
    "    d = yf.Ticker(ticker)\n",
    "    df = d.history(period='max')\n",
    "#     df = d.history(start=\"2022-06-01\", end=\"2022-06-30\", interval=\"1m\")\n",
    "#     df = d.history(start=start_date, end=end_date)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = pd.Series(df.columns).str.capitalize().values\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39364902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-02</th>\n",
       "      <td>809.200012</td>\n",
       "      <td>809.200012</td>\n",
       "      <td>809.200012</td>\n",
       "      <td>809.200012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-05</th>\n",
       "      <td>811.309998</td>\n",
       "      <td>811.309998</td>\n",
       "      <td>811.309998</td>\n",
       "      <td>811.309998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-06</th>\n",
       "      <td>803.659973</td>\n",
       "      <td>803.659973</td>\n",
       "      <td>803.659973</td>\n",
       "      <td>803.659973</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-07</th>\n",
       "      <td>801.809998</td>\n",
       "      <td>801.809998</td>\n",
       "      <td>801.809998</td>\n",
       "      <td>801.809998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-08</th>\n",
       "      <td>802.070007</td>\n",
       "      <td>802.070007</td>\n",
       "      <td>802.070007</td>\n",
       "      <td>802.070007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>33223.800781</td>\n",
       "      <td>33223.800781</td>\n",
       "      <td>33223.800781</td>\n",
       "      <td>33223.800781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>34058.800781</td>\n",
       "      <td>34058.800781</td>\n",
       "      <td>34058.800781</td>\n",
       "      <td>34058.800781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>33892.601562</td>\n",
       "      <td>33892.601562</td>\n",
       "      <td>33892.601562</td>\n",
       "      <td>33892.601562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>33294.898438</td>\n",
       "      <td>33294.898438</td>\n",
       "      <td>33294.898438</td>\n",
       "      <td>33294.898438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>33891.300781</td>\n",
       "      <td>33891.300781</td>\n",
       "      <td>33891.300781</td>\n",
       "      <td>33891.300781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12987 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  Volume  \\\n",
       "Date                                                                         \n",
       "1970-01-02    809.200012    809.200012    809.200012    809.200012       0   \n",
       "1970-01-05    811.309998    811.309998    811.309998    811.309998       0   \n",
       "1970-01-06    803.659973    803.659973    803.659973    803.659973       0   \n",
       "1970-01-07    801.809998    801.809998    801.809998    801.809998       0   \n",
       "1970-01-08    802.070007    802.070007    802.070007    802.070007       0   \n",
       "...                  ...           ...           ...           ...     ...   \n",
       "2022-02-24  33223.800781  33223.800781  33223.800781  33223.800781       0   \n",
       "2022-02-25  34058.800781  34058.800781  34058.800781  34058.800781       0   \n",
       "2022-02-28  33892.601562  33892.601562  33892.601562  33892.601562       0   \n",
       "2022-03-01  33294.898438  33294.898438  33294.898438  33294.898438       0   \n",
       "2022-03-02  33891.300781  33891.300781  33891.300781  33891.300781       0   \n",
       "\n",
       "            Dividends  Stock splits  \n",
       "Date                                 \n",
       "1970-01-02          0             0  \n",
       "1970-01-05          0             0  \n",
       "1970-01-06          0             0  \n",
       "1970-01-07          0             0  \n",
       "1970-01-08          0             0  \n",
       "...               ...           ...  \n",
       "2022-02-24          0             0  \n",
       "2022-02-25          0             0  \n",
       "2022-02-28          0             0  \n",
       "2022-03-01          0             0  \n",
       "2022-03-02          0             0  \n",
       "\n",
       "[12987 rows x 7 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_historical_data('DJI')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f815955",
   "metadata": {},
   "source": [
    "沒有我們要的區間：） <br>\n",
    "改用以下的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae6988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e3cc2ec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_datareader\n",
      "  Using cached pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/anaconda3/lib/python3.7/site-packages (from pandas_datareader) (1.3.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.7/site-packages (from pandas_datareader) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.7/site-packages (from pandas_datareader) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23->pandas_datareader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23->pandas_datareader) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.23->pandas_datareader) (1.21.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pandas_datareader) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pandas_datareader) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pandas_datareader) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->pandas_datareader) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas_datareader) (1.16.0)\n",
      "Installing collected packages: pandas_datareader\n",
      "Successfully installed pandas_datareader-0.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5153707b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>33240.218750</td>\n",
       "      <td>32752.339844</td>\n",
       "      <td>33160.589844</td>\n",
       "      <td>32990.121094</td>\n",
       "      <td>533560000</td>\n",
       "      <td>32990.121094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>33272.339844</td>\n",
       "      <td>32584.759766</td>\n",
       "      <td>33156.308594</td>\n",
       "      <td>32813.230469</td>\n",
       "      <td>338210000</td>\n",
       "      <td>32813.230469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>33248.609375</td>\n",
       "      <td>32509.429688</td>\n",
       "      <td>32809.011719</td>\n",
       "      <td>33248.281250</td>\n",
       "      <td>333210000</td>\n",
       "      <td>33248.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>33135.609375</td>\n",
       "      <td>32839.210938</td>\n",
       "      <td>32986.320312</td>\n",
       "      <td>32899.699219</td>\n",
       "      <td>298570000</td>\n",
       "      <td>32899.699219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>33235.371094</td>\n",
       "      <td>32819.500000</td>\n",
       "      <td>33032.039062</td>\n",
       "      <td>32915.781250</td>\n",
       "      <td>253010000</td>\n",
       "      <td>32915.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-07</th>\n",
       "      <td>33207.449219</td>\n",
       "      <td>32641.849609</td>\n",
       "      <td>32783.031250</td>\n",
       "      <td>33180.140625</td>\n",
       "      <td>270750000</td>\n",
       "      <td>33180.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-08</th>\n",
       "      <td>33156.500000</td>\n",
       "      <td>32824.371094</td>\n",
       "      <td>33087.070312</td>\n",
       "      <td>32910.898438</td>\n",
       "      <td>270470000</td>\n",
       "      <td>32910.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>32956.730469</td>\n",
       "      <td>32267.779297</td>\n",
       "      <td>32828.621094</td>\n",
       "      <td>32272.789062</td>\n",
       "      <td>289710000</td>\n",
       "      <td>32272.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-10</th>\n",
       "      <td>32053.519531</td>\n",
       "      <td>31387.839844</td>\n",
       "      <td>32053.519531</td>\n",
       "      <td>31392.789062</td>\n",
       "      <td>362300000</td>\n",
       "      <td>31392.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-13</th>\n",
       "      <td>31144.910156</td>\n",
       "      <td>30373.720703</td>\n",
       "      <td>31144.910156</td>\n",
       "      <td>30516.740234</td>\n",
       "      <td>472290000</td>\n",
       "      <td>30516.740234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>30690.800781</td>\n",
       "      <td>30144.230469</td>\n",
       "      <td>30592.339844</td>\n",
       "      <td>30364.830078</td>\n",
       "      <td>366800000</td>\n",
       "      <td>30364.830078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-15</th>\n",
       "      <td>31011.970703</td>\n",
       "      <td>30185.080078</td>\n",
       "      <td>30570.500000</td>\n",
       "      <td>30668.529297</td>\n",
       "      <td>392670000</td>\n",
       "      <td>30668.529297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-16</th>\n",
       "      <td>30305.740234</td>\n",
       "      <td>29740.349609</td>\n",
       "      <td>30305.740234</td>\n",
       "      <td>29927.070312</td>\n",
       "      <td>442910000</td>\n",
       "      <td>29927.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-17</th>\n",
       "      <td>30167.519531</td>\n",
       "      <td>29653.289062</td>\n",
       "      <td>29912.699219</td>\n",
       "      <td>29888.779297</td>\n",
       "      <td>692830000</td>\n",
       "      <td>29888.779297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-21</th>\n",
       "      <td>30653.980469</td>\n",
       "      <td>30074.689453</td>\n",
       "      <td>30074.689453</td>\n",
       "      <td>30530.250000</td>\n",
       "      <td>376900000</td>\n",
       "      <td>30530.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-22</th>\n",
       "      <td>30777.919922</td>\n",
       "      <td>30166.009766</td>\n",
       "      <td>30352.570312</td>\n",
       "      <td>30483.130859</td>\n",
       "      <td>343490000</td>\n",
       "      <td>30483.130859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>30715.630859</td>\n",
       "      <td>30293.400391</td>\n",
       "      <td>30570.330078</td>\n",
       "      <td>30677.359375</td>\n",
       "      <td>361420000</td>\n",
       "      <td>30677.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>31517.289062</td>\n",
       "      <td>30846.939453</td>\n",
       "      <td>30846.939453</td>\n",
       "      <td>31500.679688</td>\n",
       "      <td>465480000</td>\n",
       "      <td>31500.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>31598.589844</td>\n",
       "      <td>31351.369141</td>\n",
       "      <td>31533.599609</td>\n",
       "      <td>31438.259766</td>\n",
       "      <td>309910000</td>\n",
       "      <td>31438.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>31885.089844</td>\n",
       "      <td>30934.330078</td>\n",
       "      <td>31549.050781</td>\n",
       "      <td>30946.990234</td>\n",
       "      <td>350780000</td>\n",
       "      <td>30946.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-29</th>\n",
       "      <td>31152.960938</td>\n",
       "      <td>30894.529297</td>\n",
       "      <td>31067.410156</td>\n",
       "      <td>31029.310547</td>\n",
       "      <td>269390000</td>\n",
       "      <td>31029.310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-30</th>\n",
       "      <td>30979.849609</td>\n",
       "      <td>30431.869141</td>\n",
       "      <td>30790.000000</td>\n",
       "      <td>30775.429688</td>\n",
       "      <td>394070000</td>\n",
       "      <td>30775.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>31139.349609</td>\n",
       "      <td>30487.789062</td>\n",
       "      <td>30737.769531</td>\n",
       "      <td>31097.259766</td>\n",
       "      <td>310440000</td>\n",
       "      <td>31097.259766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    High           Low          Open         Close     Volume  \\\n",
       "Date                                                                            \n",
       "2022-05-31  33240.218750  32752.339844  33160.589844  32990.121094  533560000   \n",
       "2022-06-01  33272.339844  32584.759766  33156.308594  32813.230469  338210000   \n",
       "2022-06-02  33248.609375  32509.429688  32809.011719  33248.281250  333210000   \n",
       "2022-06-03  33135.609375  32839.210938  32986.320312  32899.699219  298570000   \n",
       "2022-06-06  33235.371094  32819.500000  33032.039062  32915.781250  253010000   \n",
       "2022-06-07  33207.449219  32641.849609  32783.031250  33180.140625  270750000   \n",
       "2022-06-08  33156.500000  32824.371094  33087.070312  32910.898438  270470000   \n",
       "2022-06-09  32956.730469  32267.779297  32828.621094  32272.789062  289710000   \n",
       "2022-06-10  32053.519531  31387.839844  32053.519531  31392.789062  362300000   \n",
       "2022-06-13  31144.910156  30373.720703  31144.910156  30516.740234  472290000   \n",
       "2022-06-14  30690.800781  30144.230469  30592.339844  30364.830078  366800000   \n",
       "2022-06-15  31011.970703  30185.080078  30570.500000  30668.529297  392670000   \n",
       "2022-06-16  30305.740234  29740.349609  30305.740234  29927.070312  442910000   \n",
       "2022-06-17  30167.519531  29653.289062  29912.699219  29888.779297  692830000   \n",
       "2022-06-21  30653.980469  30074.689453  30074.689453  30530.250000  376900000   \n",
       "2022-06-22  30777.919922  30166.009766  30352.570312  30483.130859  343490000   \n",
       "2022-06-23  30715.630859  30293.400391  30570.330078  30677.359375  361420000   \n",
       "2022-06-24  31517.289062  30846.939453  30846.939453  31500.679688  465480000   \n",
       "2022-06-27  31598.589844  31351.369141  31533.599609  31438.259766  309910000   \n",
       "2022-06-28  31885.089844  30934.330078  31549.050781  30946.990234  350780000   \n",
       "2022-06-29  31152.960938  30894.529297  31067.410156  31029.310547  269390000   \n",
       "2022-06-30  30979.849609  30431.869141  30790.000000  30775.429688  394070000   \n",
       "2022-07-01  31139.349609  30487.789062  30737.769531  31097.259766  310440000   \n",
       "\n",
       "               Adj Close  \n",
       "Date                      \n",
       "2022-05-31  32990.121094  \n",
       "2022-06-01  32813.230469  \n",
       "2022-06-02  33248.281250  \n",
       "2022-06-03  32899.699219  \n",
       "2022-06-06  32915.781250  \n",
       "2022-06-07  33180.140625  \n",
       "2022-06-08  32910.898438  \n",
       "2022-06-09  32272.789062  \n",
       "2022-06-10  31392.789062  \n",
       "2022-06-13  30516.740234  \n",
       "2022-06-14  30364.830078  \n",
       "2022-06-15  30668.529297  \n",
       "2022-06-16  29927.070312  \n",
       "2022-06-17  29888.779297  \n",
       "2022-06-21  30530.250000  \n",
       "2022-06-22  30483.130859  \n",
       "2022-06-23  30677.359375  \n",
       "2022-06-24  31500.679688  \n",
       "2022-06-27  31438.259766  \n",
       "2022-06-28  30946.990234  \n",
       "2022-06-29  31029.310547  \n",
       "2022-06-30  30775.429688  \n",
       "2022-07-01  31097.259766  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_datareader import data, wb\n",
    "DJIA = data.DataReader(name='^DJI', data_source='yahoo',start=start_date, end=end_date)\n",
    "DJIA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43c2b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DJIA.to_csv(\"DJIA_price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f837b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
